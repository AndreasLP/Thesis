{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, os\n",
    "\n",
    "# import torch \n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13918095: 1-4\n",
    "13918148: 5-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"13882878\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id in run_file and run_file.endswith(\"unsigned\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\" : int(s.split(\"-\")[1])} for s in summary_iterators.keys()}\n",
    "df_meta_info = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "\n",
    "group1 = df_meta_info[df_meta_info[\"run_index\"] <= 11]\n",
    "group2 = df_meta_info[df_meta_info[\"run_index\"] > 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in group1.groupby(\"run_index\")}\n",
    "\n",
    "results1_train = pd.DataFrame({\"{:.2e}\".format(lr) : results1_all[i][\"train/mean/aggregate\"] for i,lr in enumerate(np.logspace(-5,-2,11)[:11],1)})\n",
    "results1_val = pd.DataFrame({\"{:.2e}\".format(lr) : results1_all[i][\"val/mean/aggregate\"] for i,lr in enumerate(np.logspace(-5,-2,11)[:11],1)})\n",
    "\n",
    "results2_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in group2.groupby(\"run_index\")}\n",
    "\n",
    "results2_train = pd.DataFrame({\"{:.2e}\".format(lr) : results2_all[i][\"train/mean/aggregate\"] for i,lr in enumerate(np.logspace(-5,-2,11)[:11],12)})\n",
    "results2_val = pd.DataFrame({\"{:.2e}\".format(lr) : results2_all[i][\"val/mean/aggregate\"] for i,lr in enumerate(np.logspace(-5,-2,11)[:11],12)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1_train.to_pickle(\"cache/results1_train.pkl\")\n",
    "results1_val.to_pickle(\"cache/results1_val.pkl\")\n",
    "results2_train.to_pickle(\"cache/results2_train.pkl\")\n",
    "results2_val.to_pickle(\"cache/results2_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results1 use\n",
    "    hyperparameters['strides'] =        [1, 3, 1, 1]  + [1]\n",
    "    hyperparameters['paddings'] =       [200*3, 0, 0, 0]  + [0]\n",
    "    hyperparameters['kernel_lenghts'] = [400*3+1, 3*3, 7, 7] + [3]\n",
    "    hyperparameters['channels'] =       [128, 256, 64, 32]\n",
    "while results2 use\n",
    "    hyperparameters['strides'] =        [1, 3, 1, 1]  + [1]\n",
    "    hyperparameters['paddings'] =       [200*3, 0, 0, 0]  + [0]\n",
    "    hyperparameters['kernel_lenghts'] = [400*3+1,3*3, 7, 7] + [3]\n",
    "    hyperparameters['channels'] =       [64, 128, 64, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"13917855\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators3 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id in run_file and run_file.endswith(\"unsigned\")}\n",
    "\n",
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators3.keys()}\n",
    "df_meta_info3 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "\n",
    "results3_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators3[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators3[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info3.groupby(\"run_index\")}\n",
    "\n",
    "results3_train = pd.DataFrame({i : results3_all[i][\"train/mean/aggregate\"] for i in results3_all.keys()})\n",
    "results3_val = pd.DataFrame({i : results3_all[i][\"val/mean/aggregate\"] for i in results3_all.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3_train.to_pickle(\"cache/results3_train.pkl\")\n",
    "results3_val.to_pickle(\"cache/results3_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id_1 = \"13918095\"\n",
    "run_id_2 = \"13918148\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators6_1 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id_1 in run_file and run_file.endswith(\"unsigned\")}\n",
    "summary_iterators6_2 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id_2 in run_file and run_file.endswith(\"unsigned\")}\n",
    "\n",
    "df_meta_info6_1 = pd.DataFrame.from_dict({s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators6_1.keys()}, orient=\"index\")\n",
    "df_meta_info6_2 = pd.DataFrame.from_dict({s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators6_2.keys()}, orient=\"index\")\n",
    "\n",
    "df_meta_info6_1 = df_meta_info6_1.loc[df_meta_info6_1.run_index <= 4]\n",
    "df_meta_info6_2 = df_meta_info6_2.loc[df_meta_info6_2.run_index >= 5]\n",
    "\n",
    "results6_1_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators6_1[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators6_1[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info6_1.groupby(\"run_index\")}\n",
    "results6_1_train = pd.DataFrame({i : results6_1_all[i][\"train/mean/aggregate\"] for i in results6_1_all.keys()})\n",
    "results6_1_val = pd.DataFrame({i : results6_1_all[i][\"val/mean/aggregate\"] for i in results6_1_all.keys()})\n",
    "results6_2_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators6_2[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators6_2[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info6_2.groupby(\"run_index\")}\n",
    "results6_2_train = pd.DataFrame({i : results6_2_all[i][\"train/mean/aggregate\"] for i in results6_2_all.keys()})\n",
    "results6_2_val = pd.DataFrame({i : results6_2_all[i][\"val/mean/aggregate\"] for i in results6_2_all.keys()})\n",
    "\n",
    "results6_train = results6_1_train.join(results6_2_train)\n",
    "results6_val = results6_1_val.join(results6_2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results6_train.to_pickle(\"cache/results6_train.pkl\")\n",
    "results6_val.to_pickle(\"cache/results6_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate should not be changed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id_3 = \"13918659\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators7_3 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id_3 in run_file and run_file.endswith(\"unsigned\")}\n",
    "\n",
    "df_meta_info7_3 = pd.DataFrame.from_dict({s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators7_3.keys()}, orient=\"index\")\n",
    "\n",
    "results7_3_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators7_3[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators7_3[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info7_3.groupby(\"run_index\")}\n",
    "\n",
    "results7_3_train = pd.DataFrame({i : results7_3_all[i][\"train/mean/aggregate\"] for i in results7_3_all.keys()})\n",
    "results7_3_val = pd.DataFrame({i : results7_3_all[i][\"val/mean/aggregate\"] for i in results7_3_all.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id_1 = \"13918151\"\n",
    "run_id_2 = \"13918499\"\n",
    "run_id_3 = \"13918659\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators7_1 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id_1 in run_file and run_file.endswith(\"unsigned\")}\n",
    "summary_iterators7_2 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id_2 in run_file and run_file.endswith(\"unsigned\")}\n",
    "summary_iterators7_3 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id_3 in run_file and run_file.endswith(\"unsigned\")}\n",
    "\n",
    "\n",
    "df_meta_info7_1 = pd.DataFrame.from_dict({s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators7_1.keys()}, orient=\"index\")\n",
    "df_meta_info7_2 = pd.DataFrame.from_dict({s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators7_2.keys()}, orient=\"index\")\n",
    "df_meta_info7_3 = pd.DataFrame.from_dict({s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators7_3.keys()}, orient=\"index\")\n",
    "\n",
    "\n",
    "results7_1_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators7_1[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators7_1[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info7_1.groupby(\"run_index\")}\n",
    "results7_1_train = pd.DataFrame({i : results7_1_all[i][\"train/mean/aggregate\"] for i in results7_1_all.keys()})\n",
    "results7_1_val = pd.DataFrame({i : results7_1_all[i][\"val/mean/aggregate\"] for i in results7_1_all.keys()})\n",
    "results7_2_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators7_2[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators7_2[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info7_2.groupby(\"run_index\")}\n",
    "results7_2_train = pd.DataFrame({i : results7_2_all[i][\"train/mean/aggregate\"] for i in results7_2_all.keys()})\n",
    "results7_2_val = pd.DataFrame({i : results7_2_all[i][\"val/mean/aggregate\"] for i in results7_2_all.keys()})\n",
    "results7_3_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators7_3[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators7_3[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info7_3.groupby(\"run_index\")}\n",
    "results7_3_train = pd.DataFrame({i : results7_3_all[i][\"train/mean/aggregate\"] for i in results7_3_all.keys()})\n",
    "results7_3_val = pd.DataFrame({i : results7_3_all[i][\"val/mean/aggregate\"] for i in results7_3_all.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results7_train = results7_1_train.join(results7_2_train).join(results7_3_train)\n",
    "results7_val = results7_1_val.join(results7_2_val).join(results7_3_val)\n",
    "results7_train.to_pickle(\"cache/results7_train.pkl\")\n",
    "results7_val.to_pickle(\"cache/results7_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"13918897\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators8 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id in run_file and run_file.endswith(\"unsigned\")}\n",
    "\n",
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators8.keys()}\n",
    "df_meta_info8 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "\n",
    "results8_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators8[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators8[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info8.groupby(\"run_index\")}\n",
    "\n",
    "results8_train = pd.DataFrame({i : results8_all[i][\"train/mean/aggregate\"] for i in results8_all.keys()})\n",
    "results8_val = pd.DataFrame({i : results8_all[i][\"val/mean/aggregate\"] for i in results8_all.keys()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results8_train.to_pickle(\"cache/results8_train.pkl\")\n",
    "results8_val.to_pickle(\"cache/results8_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"13918954\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators9 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id in run_file and run_file.endswith(\"unsigned\")}\n",
    "\n",
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators9.keys()}\n",
    "df_meta_info9 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "\n",
    "results9_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators9[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators9[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info9.groupby(\"run_index\")}\n",
    "\n",
    "results9_train = pd.DataFrame({i : results9_all[i][\"train/mean/aggregate\"] for i in results9_all.keys()})\n",
    "results9_val = pd.DataFrame({i : results9_all[i][\"val/mean/aggregate\"] for i in results9_all.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "results9_train.to_pickle(\"cache/results9_train.pkl\")\n",
    "results9_val.to_pickle(\"cache/results9_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are quite regular across seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try using longer kernels and a bit of architecture changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"13918955\"\n",
    "run_id = \"13919177\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators10_1 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id_1 in run_file and run_file.endswith(\"unsigned\")}\n",
    "summary_iterators10_2 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id_2 in run_file and run_file.endswith(\"unsigned\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators10_1.keys()}\n",
    "\n",
    "df_meta_info10 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "df_meta_info10 = df_meta_info10.loc[(df_meta_info10.run_index != 21) & (df_meta_info10.run_index != 24)]\n",
    "\n",
    "results10_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators10_1[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators10_1[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info10.groupby(\"run_index\")}\n",
    "\n",
    "results10_train_1 = pd.DataFrame({i : results10_all[i][\"train/mean/aggregate\"] for i in results10_all.keys()})\n",
    "results10_val_1 = pd.DataFrame({i : results10_all[i][\"val/mean/aggregate\"] for i in results10_all.keys()})\n",
    "\n",
    "\n",
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators10_2.keys()}\n",
    "\n",
    "\n",
    "df_meta_info10 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "df_meta_info10 = df_meta_info10.loc[(df_meta_info10.run_index == 21) | (df_meta_info10.run_index == 24)]\n",
    "\n",
    "results10_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators10_2[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators10_2[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info10.groupby(\"run_index\")}\n",
    "\n",
    "results10_train_2 = pd.DataFrame({i : results10_all[i][\"train/mean/aggregate\"] for i in results10_all.keys()})\n",
    "results10_val_2 = pd.DataFrame({i : results10_all[i][\"val/mean/aggregate\"] for i in results10_all.keys()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "results10_train = results10_train_1.join(results10_train_2)\n",
    "results10_val = results10_val_1.join(results10_val_2)\n",
    "results10_train = results10_train.reindex(sorted(results10_train.columns), axis=1)\n",
    "results10_val = results10_val.reindex(sorted(results10_val.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "results10_train.to_pickle(\"cache/results10_train.pkl\")\n",
    "results10_val.to_pickle(\"cache/results10_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"13919178\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators12 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id in run_file and run_file.endswith(\"unsigned\")}\n",
    "\n",
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators12.keys()}\n",
    "df_meta_info12 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "\n",
    "results12_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators12[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators12[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info12.groupby(\"run_index\")}\n",
    "\n",
    "results12_train = pd.DataFrame({i : results12_all[i][\"train/mean/aggregate\"] for i in results12_all.keys()})\n",
    "results12_val = pd.DataFrame({i : results12_all[i][\"val/mean/aggregate\"] for i in results12_all.keys()})\n",
    "\n",
    "results12_train.columns = [\"ELU\", \"Leaky-ReLU\", \"RReLU\", \"PReLU\"]\n",
    "results12_val.columns = [\"ELU\", \"Leaky-ReLU\", \"RReLU\", \"PReLU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "results12_train.to_pickle(\"cache/results12_train.pkl\")\n",
    "results12_val.to_pickle(\"cache/results12_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the following activation functions\n",
    "\n",
    "1. ReLU\n",
    "2. ELU\n",
    "3. LeakyReLU\n",
    "4. RReLU\n",
    "5. PReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "run_id_1 = \"13919181\"\n",
    "run_id_2 = \"13931824\"\n",
    "run_id_3 = \"13934900\"\n",
    "summary_iterators13_1 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id_1 in run_file and run_file.endswith(\"unsigned\")}\n",
    "summary_iterators13_2 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id_2 in run_file and run_file.endswith(\"unsigned\")}\n",
    "summary_iterators13_3 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id_3 in run_file and run_file.endswith(\"unsigned\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators13_1.keys()}\n",
    "df_meta_info13_1 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators13_2.keys()}\n",
    "df_meta_info13_2 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators13_3.keys()}\n",
    "df_meta_info13_3 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "\n",
    "df_meta_info13_1 = df_meta_info13_1.loc[df_meta_info13_1[\"cv_index\"] < 14]\n",
    "df_meta_info13 = pd.concat((df_meta_info13_1,df_meta_info13_2,df_meta_info13_3))\n",
    "\n",
    "summary_iterators13 = summary_iterators13_1\n",
    "summary_iterators13.update(summary_iterators13_2)\n",
    "summary_iterators13.update(summary_iterators13_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators13[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators13[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info13.groupby(\"run_index\")}\n",
    "\n",
    "results13_train = pd.DataFrame({i : results13_all[i][\"train/mean/aggregate\"] for i in results13_all.keys()})\n",
    "results13_val = pd.DataFrame({i : results13_all[i][\"val/mean/aggregate\"] for i in results13_all.keys()})\n",
    "\n",
    "results13_train.columns = [\"{:.2e}\".format(i) for i in np.logspace(-7,0,8)]\n",
    "results13_val.columns = [\"{:.2e}\".format(i) for i in np.logspace(-7,0,8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13_train.to_pickle(\"cache/results13_train.pkl\")\n",
    "results13_val.to_pickle(\"cache/results13_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 3.162277660168379e-07 as weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 0.1778920167412 as steps every 100 epoch such that the final 100 epochs are trained with a learning rate of ~1e-6 \n",
    "\n",
    "Hand tuned to a factor 0.4 at 150 and 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"13919182\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators14 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id in run_file and run_file.endswith(\"unsigned\")}\n",
    "\n",
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators14.keys()}\n",
    "df_meta_info14 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "\n",
    "results14_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators14[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators14[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info14.groupby(\"run_index\")}\n",
    "\n",
    "results14_train = pd.DataFrame({i : results14_all[i][\"train/mean/aggregate\"] for i in results14_all.keys()})\n",
    "results14_val = pd.DataFrame({i : results14_all[i][\"val/mean/aggregate\"] for i in results14_all.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_optimizer = { 1  : \"SGD\", 2  : \"nesterov\", 3  : \"AdamW\", 4  : \"AMSGrad\", 5  : \"Adadelta\", 6  : \"Adagrad\", \n",
    "    7 : \"Adamax\", 8 : \"ASGD\", 9 : \"Adam\", 10 : \"NAdam\", 11 : \"RAdam\", 12 : \"RMSprop\", 13 : \"Rprop\", 14 : \"LBFGS\"}\n",
    "results14_train.columns = [index_to_optimizer[col] for col in results14_train.columns]\n",
    "results14_val.columns = results14_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "results14_train.to_pickle(\"cache/results14_train.pkl\")\n",
    "results14_val.to_pickle(\"cache/results14_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimiser, milestones=[150,250], gamma=0.40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the best optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"13936781\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators15 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id in run_file and run_file.endswith(\"unsigned\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators15.keys()}\n",
    "df_meta_info15 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "# df_meta_info15 = df_meta_info15.loc[df_meta_info15[\"run_index\"] <= 5]\n",
    "\n",
    "results15_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators15[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators15[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info15.groupby(\"run_index\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_optimizer = {\n",
    "                1  : \"SGD\", 2  : \"nesterov\", 3  : \"AdamW\", 4  : \"AMSGrad\",\n",
    "                5  : \"Adadelta\", 6  : \"Adagrad\", 7  : \"Adamax\", 8  : \"ASGD\",\n",
    "                9  : \"Adam\", 10 : \"NAdam\", 11 : \"RAdam\", 12 : \"RMSprop\",\n",
    "                13 : \"Rprop\", 14 : \"LBFGS\",\n",
    "            }\n",
    "\n",
    "results15_train = pd.DataFrame({i : results15_all[i][\"train/mean/aggregate\"] for i in results15_all.keys()})\n",
    "results15_val = pd.DataFrame({i : results15_all[i][\"val/mean/aggregate\"] for i in results15_all.keys()})\n",
    "\n",
    "results15_train.columns = [index_to_optimizer[col] for col in results15_train.columns]\n",
    "results15_val.columns = [index_to_optimizer[col] for col in results15_val.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results15_train.to_pickle(\"cache/results15_train.pkl\")\n",
    "results15_val.to_pickle(\"cache/results15_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"13914271\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators16 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id in run_file and run_file.endswith(\"unsigned\")}\n",
    "\n",
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators16.keys()}\n",
    "df_meta_info16 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "\n",
    "results16_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators16[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators16[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info16.groupby(\"run_index\")}\n",
    "\n",
    "results16_train = pd.DataFrame({i : results16_all[i][\"train/mean/aggregate\"] for i in results16_all.keys()})\n",
    "results16_val = pd.DataFrame({i : results16_all[i][\"val/mean/aggregate\"] for i in results16_all.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "results16_train.to_pickle(\"cache/results16_train.pkl\")\n",
    "results16_val.to_pickle(\"cache/results16_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"13936930\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators17 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id in run_file and run_file.endswith(\"unsigned\")}\n",
    "\n",
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators17.keys()}\n",
    "df_meta_info17 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "\n",
    "results17_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators17[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators17[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info17.groupby(\"run_index\")}\n",
    "\n",
    "results17_train = pd.DataFrame({i : results17_all[i][\"train/mean/aggregate\"] for i in results17_all.keys()})\n",
    "results17_val = pd.DataFrame({i : results17_all[i][\"val/mean/aggregate\"] for i in results17_all.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results17_train.to_pickle(\"cache/results17_train.pkl\")\n",
    "results17_val.to_pickle(\"cache/results17_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"13934908\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators18 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id in run_file and run_file.endswith(\"unsigned\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators18.keys()}\n",
    "df_meta_info18 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "\n",
    "results18_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators18[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators18[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info18.groupby(\"run_index\")}\n",
    "\n",
    "results18_train = pd.DataFrame({i : results18_all[i][\"train/mean/aggregate\"] for i in results18_all.keys()})\n",
    "results18_val = pd.DataFrame({i : results18_all[i][\"val/mean/aggregate\"] for i in results18_all.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results18_train.to_pickle(\"cache/results18_train.pkl\")\n",
    "results18_val.to_pickle(\"cache/results18_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"13936780\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators19 = {run_file[47:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id in run_file and run_file.endswith(\"unsigned\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":int(s.split(\"-\")[1])} for s in \n",
    "    summary_iterators19.keys()}\n",
    "df_meta_info19 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "\n",
    "results19_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators19[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators19[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info19.groupby(\"run_index\")}\n",
    "\n",
    "results19_train = pd.DataFrame({i : results19_all[i][\"train/mean/aggregate\"] for i in results19_all.keys()})\n",
    "results19_val = pd.DataFrame({i : results19_all[i][\"val/mean/aggregate\"] for i in results19_all.keys()})\n",
    "\n",
    "results19_all_max = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators19[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).max(1) for tag in summary_iterators19[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info19.groupby(\"run_index\")}\n",
    "\n",
    "results19_train_max = pd.DataFrame({i : results19_all_max[i][\"train/max/aggregate\"] for i in results19_all_max.keys()})\n",
    "results19_val_max = pd.DataFrame({i : results19_all_max[i][\"val/max/aggregate\"] for i in results19_all_max.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results19_train.to_pickle(\"cache/results19_train.pkl\")\n",
    "results19_val.to_pickle(\"cache/results19_val.pkl\")\n",
    "results19_train_max.to_pickle(\"cache/results19_train_max.pkl\")\n",
    "results19_val_max.to_pickle(\"cache/results19_val_max.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"13936779\"\n",
    "folder =\"/work1/s174505/Thesis/runs/\"\n",
    "summary_iterators20 = {run_file[46:] : EventAccumulator(os.path.join(folder, run_file)).Reload() for run_file in os.listdir(folder)\n",
    "    if run_id in run_file and run_file.endswith(\"unsigned\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'13936779_0_unsigned': <tensorboard.backend.event_processing.event_accumulator.EventAccumulator at 0x7f22175cc1f0>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_iterators20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_info = {s : {\"run_id\" : s.split(\"_\")[0], \"run_index\" : int(s.split(\"_\")[1]), \"cv_index\":-1} for s in \n",
    "    summary_iterators20.keys()}\n",
    "df_meta_info20 = pd.DataFrame.from_dict(meta_info, orient=\"index\")\n",
    "\n",
    "results20_all = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators20[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).mean(1) for tag in summary_iterators20[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info20.groupby(\"run_index\")}\n",
    "\n",
    "results20_train = pd.DataFrame({i : results20_all[i][\"train/mean/aggregate\"] for i in results20_all.keys()})\n",
    "results20_val = pd.DataFrame({i : results20_all[i][\"val/mean/aggregate\"] for i in results20_all.keys()})\n",
    "\n",
    "results20_all_max = {grp[0] : pd.DataFrame({tag : pd.DataFrame({\n",
    "        s : [e.value for e in summary_iterators20[s].Scalars(tag)] for s in grp[1].index\n",
    "    }).max(1) for tag in summary_iterators20[grp[1].index[0]].Tags()[\"scalars\"]}) for grp in df_meta_info20.groupby(\"run_index\")}\n",
    "\n",
    "results20_train_max = pd.DataFrame({i : results20_all_max[i][\"train/max/aggregate\"] for i in results20_all_max.keys()})\n",
    "results20_val_max = pd.DataFrame({i : results20_all_max[i][\"val/max/aggregate\"] for i in results20_all_max.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results20_train.to_pickle(\"cache/results20_train.pkl\")\n",
    "results20_val.to_pickle(\"cache/results20_val.pkl\")\n",
    "results20_train_max.to_pickle(\"cache/results20_train_max.pkl\")\n",
    "results20_val_max.to_pickle(\"cache/results20_val_max.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
